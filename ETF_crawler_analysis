import requests
import pandas as pd
import logging
import time
import os
from datetime import datetime
import openpyxl

class ETFCrawler:
    def __init__(self):
        # Basic Configuration
        self.BASE_URL = "https://www.jisilu.cn/data/etf/etf_list/"
        self.HEADERS = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "X-Requested-With": "XMLHttpRequest",
            "Referer": "https://www.jisilu.cn/data/etf/",
            "Accept": "application/json"
        }
        self.session = requests.Session()
        
        # File Configuration
        self.DATA_DIR = "etf_data1"
        self.SUMMARY_FILE = "index_summary.xlsx"  # 改为xlsx
        self.CHANGES_FILE = "daily_changes.xlsx"  # 改为xlsx
        self.CUMULATIVE_FILE = "cumulative_changes.xlsx"  # 改为xlsx
        self.MANUAL_EXCEL = "manual_index_data.xlsx"
        self.FIRST_DAY_SHARES = "first_day_shares.xlsx"
        
        # Create Data Directory
        os.makedirs(self.DATA_DIR, exist_ok=True)

    def make_request(self, max_retries=3):
        """Request Function with Retry Mechanism"""
        for attempt in range(max_retries):
            try:
                params = {
                    "__jsl": f"LST_t={int(time.time()*1000)}",
                    "rp": 9999
                }
                response = self.session.get(
                    self.BASE_URL,
                    params=params,
                    timeout=15
                )
                response.raise_for_status()
                return response
            except requests.exceptions.SSLError as e:
                logging.warning(f"SSL Error (Attempt {attempt+1}/{max_retries}）: {e}")
                time.sleep(2)
            except requests.exceptions.RequestException as e:
                logging.warning(f"Request Failed (Attempt {attempt+1}/{max_retries}）: {e}")
                time.sleep(2)
        return None

    def fetch_data(self):
        """Fetching and Validating ETF Data"""
        logging.info("Initiating ETF Data Retrieval...")
        response = self.make_request()
        if not response:
            logging.error("Request Failure")
            return pd.DataFrame()

        try:
            data = response.json()
            if not isinstance(data, dict) or "rows" not in data:
                logging.error("Response Data Structure Exception")
                return pd.DataFrame()
            records = []
            for item in data["rows"]:
                cell = item.get("cell", {})
                try:
                    records.append({
                        "代码": str(cell.get("fund_id", "")).strip(),
                        "名称": cell.get("fund_nm", ""),
                        "净值": float(cell.get("fund_nav", 0)),
                        "份额(万份)": float(cell.get("amount", 0)),
                        "指数": cell.get("index_nm", "")
                    })
                except (ValueError, TypeError) as e:
                    logging.warning(f"Data Parsing Exception: {e}，Skip Record")
            
            df = pd.DataFrame(records)
            if df.empty:
                logging.error("Retrieved ETF Data is Empty")
                return df
            
            # Data Validation
            zero_nav = df[df['净值'] == 0]
            zero_share = df[df['份额(万份)'] == 0]
            if not zero_nav.empty:
                logging.warning(f"{len(zero_nav)}Zero Net Value Records Found")
            if not zero_share.empty:
                logging.warning(f"{len(zero_share)}Zero Net Value Records Found")
            
            logging.info(f"{len(df)}Successful Data Retrieval，Net Value Range[{df['净值'].min():.2f}-{df['净值'].max():.2f}]")
            return df
            
        except Exception as e:
            logging.error(f"Data Processing Failure: {e}")
            return pd.DataFrame()

    def save_daily_data(self, df):
        """Save Daily Data as Excel"""
        if df.empty:
            logging.warning("No Data Available for Saving")
            return False
        
        date_str = datetime.now().strftime("%Y-%m-%d")
        filename = os.path.join(self.DATA_DIR, f"ETF_{date_str}.xlsx")
        
        try:
            df.to_excel(
                filename,
                index=False,
                engine='openpyxl'
            )
            logging.info(f"Data Successfully Saved to Excel File {filename}")
            return True
        except Exception as e:
            logging.error(f"Save Operation Failed: {e}")
            return False

    def get_previous_shares(self):
        """Retrieve Historical Share Data with Strict Error Checking"""
        def safe_read_excel(path):
            try:
                df = pd.read_excel(
                    path,
                    engine='openpyxl',
                    dtype={'代码': str}, 
                    usecols=['代码', '份额(万份)']  
                )
                df['代码'] = df['代码'].str.strip()
                return df
            except Exception as e:
                print(f"Failed to Read Excel File [{path}]: {str(e)}")
                return None

        # Prioritize-Reading Manually Prepared Initial Data
        if os.path.exists(self.FIRST_DAY_SHARES):
            df = safe_read_excel(self.FIRST_DAY_SHARES)
            if df is not None:
                print("√ Successfully Read Initial Share Data")
                return df[['代码', '份额(万份)']]

        # Attempt to Read Historical Excel File
        date_str = datetime.now().strftime("%Y-%m-%d")
        excel_files = sorted(
            [f for f in os.listdir(self.DATA_DIR) 
             if f.startswith("ETF_") and f.endswith(".xlsx") and not f.endswith(f"{date_str}.xlsx")],
            reverse=True
        )
        if excel_files:
            try:
                df = pd.read_excel(
                    os.path.join(self.DATA_DIR, excel_files[0]),
                    engine='openpyxl',
                    dtype={'代码': str},
                    usecols=['代码', '份额(万份)']
                )
                df['代码'] = df['代码'].str.strip()
                print(f"√ Read Historical Excel Data: {excel_files[0]}")
                return df[['代码', '份额(万份)']]
            except Exception as e:
                print(f"Failed to Read Excel File: {str(e)}")

        print("× No Available Historical Data")
        return None

    def calculate_daily_changes(self, today_df):
        """Calculate Scale Variation"""
        prev_shares = self.get_previous_shares()
        
        if prev_shares is None:
            logging.warning("No Historical Share Data; Calculate Today's Scale")
            today_df['规模变化'] = today_df['净值'] * today_df['份额(万份)']
            return today_df[['代码', '指数', '规模变化']]
        
        # Data Preprocessing
        today_df['代码'] = today_df['代码'].astype(str).str.strip()
        prev_shares['代码'] = prev_shares['代码'].astype(str).str.strip()
        
        # Debug Output
        logging.info(f"Top 5 Records of Today's Data:\n{today_df[['代码','净值','份额(万份)']].head()}")
        logging.info(f"Top 5 Records of Historical Shares:\n{prev_shares.head()}")
        
        # Merge Data (Retain All Records for Debugging)
        merged = pd.merge(
            today_df[['代码', '净值', '份额(万份)', '指数']],
            prev_shares,
            on='代码',
            how='outer',
            suffixes=('_today', '_yesterday'),
            indicator=True
        ).reset_index(drop=True)
        
        # Analyze Merge Results
        merge_stats = merged['_merge'].value_counts()
        logging.info(f"Merge Status:\n{merge_stats}")
        
        # Calculate Changes (Only for ETFs Present Today)
        result = merged[merged['_merge'].isin(['both', 'left_only'])]
        result['规模变化'] = result['净值'] * (
            result['份额(万份)_today'].fillna(0) - 
            result['份额(万份)_yesterday'].fillna(0)
        )
        
        # Mark New ETFs
        new_etfs = result['_merge'] == 'left_only'
        if new_etfs.any():
            logging.info(f"Discover{new_etfs.sum()}New ETFs")
        
        # Result Validation
        zero_changes = result[result['规模变化'] == 0]
        if len(zero_changes) > 0.5 * len(result):
            logging.warning(f"发现{len(zero_changes)}条零变化记录，请检查数据！")
            sample = zero_changes.head().merge(
                today_df[['代码', '净值', '份额(万份)']], 
                on='代码'
            )
            logging.info(f"Zero Change Example:\n{sample}")
        
        return result[['代码', '指数', '规模变化']]

    def aggregate_by_index(self, changes_df):
        """Index Aggregation"""
        if changes_df is None or changes_df.empty:
            logging.error("No Significant Change Data")
            return pd.DataFrame(columns=['指数', '规模变化'])
        try:
            # Reading Reference Index
            ref_df = pd.read_excel(
                self.MANUAL_EXCEL,
                engine='openpyxl',
                header=0
            )
            valid_indices = ref_df['指数'].dropna().unique()
            # Marking New Indices
            new_indices_mask = ~changes_df['指数'].isin(valid_indices)
            new_indices = changes_df[new_indices_mask]['指数'].unique()
            if len(new_indices) > 0:
                logging.info(f"Discovering{len(new_indices)}New Indices: {new_indices}")
            
                # Preparing Data for New Indices
                new_rows = []
                for idx in new_indices:
                    # AUM Change for New Indices = ∑(NAV of ETF × Shares)
                    etf_data = changes_df[changes_df['指数'] == idx]
                    total_change = etf_data['规模变化'].sum()
                    new_rows.append({
                        '指数': idx,
                        '规模变化': total_change
                    })
            
                # Adding New Indices to manual_index_data.xlsx
                new_indices_df = pd.DataFrame({
                    '指数': new_indices,
                    '份额(万份)': 0,
                    '计数项': None
                })
            
                # Adjusting Column Order According to Original Format
                if len(ref_df.columns) == 3:
                    new_indices_df = new_indices_df[['计数项', '份额(万份)', '指数']]

                # Merging New and Old Data with Deduplication
                updated_ref_df = pd.concat([ref_df, new_indices_df]).drop_duplicates(subset=['指数'])
                # Saving the Updated File
                updated_ref_df.to_excel(
                    self.MANUAL_EXCEL,
                    index=False,
                    engine='openpyxl'
                )
                logging.info(f"{len(new_indices)}New Indices have been added to{self.MANUAL_EXCEL}")
            
                # Immediate Reload of Active Index List
                valid_indices = updated_ref_df['指数'].dropna().unique()
        
            # Final Aggregated Results - Including Existing and New Indices
            aggregated = changes_df.groupby('指数', as_index=False)['规模变化'].sum().reset_index(drop=True)
            aggregated = aggregated.sort_values('指数', key=lambda x: x.str.strip())
            
            # Additional Data Validation
            logging.info(f"Number of Indices After Aggregation: {len(aggregated)}")
            logging.info(f"Top 5 Indices After Aggregation: {aggregated['指数'].head().tolist()}")
            
            return aggregated
        
        except Exception as e:
            logging.error(f"Index Aggregation Failure: {e}")
            # Reverting to Simple Aggregation
            return changes_df.groupby('指数', as_index=False)['规模变化'].sum().reset_index(drop=True)

    def update_summary_files(self, aggregated_changes):
        """Update Summary File"""
        if aggregated_changes is None or aggregated_changes.empty:
            logging.error("No Valid Aggregated Data")
            return False
        
        date_str = datetime.now().strftime("%Y-%m-%d")
        daily_path = os.path.join(self.DATA_DIR, self.CHANGES_FILE)
        cumulative_path = os.path.join(self.DATA_DIR, self.CUMULATIVE_FILE)

        try:
            # Create Change Dictionary for Precise Mapping
            change_dict = dict(zip(aggregated_changes['指数'], aggregated_changes['规模变化']))
            
            # Read or Create Daily Change File
            if os.path.exists(daily_path):
                daily_df = pd.read_excel(daily_path, engine='openpyxl')
                
                # Format Existing Date Column Names
                date_cols = [col for col in daily_df.columns if col != '指数']
                for col in date_cols:
                    if isinstance(col, datetime):
                        new_col_name = col.strftime("%Y-%m-%d")
                        daily_df = daily_df.rename(columns={col: new_col_name})
                
                # Handle New Indices: Insert New Rows in the Correct Position
                existing_indices = set(daily_df['指数'])
                new_indices = set(aggregated_changes['指数']) - existing_indices
                
                if new_indices:
                    logging.info(f"发现{len(new_indices)}个新增指数: {sorted(new_indices)}")
                    
                    # Create Complete Row Data for New Indices
                    new_rows_data = []
                    for idx in new_indices:
                        row_data = {'指数': idx}
                        # Fill All Historical Dates with Zeros
                        for existing_date in daily_df.columns:
                            if existing_date != '指数':
                                row_data[existing_date] = 0
                        new_rows_data.append(row_data)
                    
                    new_rows_df = pd.DataFrame(new_rows_data)
                    daily_df = pd.concat([daily_df, new_rows_df], ignore_index=True)

                # Check for Existing Data for the Current Day
                if date_str in daily_df.columns:
                    logging.warning(f"覆盖已有日期数据: {date_str}")
                
                # Update Today's Data Using Exact Mapping
                daily_df[date_str] = daily_df['指数'].map(change_dict).fillna(0)
                
            else:
                # Create a New Daily Change File
                daily_df = pd.DataFrame({
                    '指数': aggregated_changes['指数'],
                    date_str: aggregated_changes['规模变化']
                })
                daily_df = daily_df.sort_values('指数').reset_index(drop=True)

            # Read or Create Cumulative File
            if os.path.exists(cumulative_path):
                cum_df = pd.read_excel(cumulative_path, engine='openpyxl', dtype={'指数': str})
                
                # Ensure Cumulative File Contains All Indices (Including New Ones)
                missing_indices = set(daily_df['指数']) - set(cum_df['指数'])
                if missing_indices:

                    logging.info(f"Add{len(missing_indices)}New Indices to Cumulative File")
                    new_rows_data = []
                    for idx in missing_indices:
                        row_data = {'指数': idx}
                        # Fill Existing Cumulative Columns with Zeros
                        for col in cum_df.columns:
                            if col != '指数':
                                row_data[col] = 0
                        new_rows_data.append(row_data)
                    
                    new_rows_df = pd.DataFrame(new_rows_data)
                    cum_df = pd.concat([cum_df, new_rows_df], ignore_index=True)
                
                # Ensure Consistent Order of Indices
                cum_df = cum_df.sort_values('指数').reset_index(drop=True)
                
            else:
                cum_df = pd.DataFrame({'指数': daily_df['指数'].copy()})

            # Calculate Cumulative Changes
            date_cols = [col for col in daily_df.columns if col != '指数']
            
            # Ensure Indices Order is Identical in Two DataFrames
            daily_df = daily_df.sort_values('指数').reset_index(drop=True)
            cum_df = cum_df.sort_values('指数').reset_index(drop=True)
            
            # Validate Index Alignment
            if not daily_df['指数'].equals(cum_df['指数']):
                logging.error("daily_df和cum_df的指数不一致！")
                logging.info(f"daily_df指数: {daily_df['指数'].tolist()[:10]}")
                logging.info(f"cum_df指数: {cum_df['指数'].tolist()[:10]}")
                logging.info(f"cum_df指数: {cum_df['指数'].tolist()[:10]}")
                
                # Index Alignment
                cum_df = cum_df.set_index('指数').reindex(daily_df['指数']).reset_index()
                cum_df['指数'] = daily_df['指数']
            
            # Calculate Cumulative Changes
            days = len(date_cols)
            if days > 0:
                # Calculate Cumulative Changes for Existing Dates in daily_df
                valid_date_cols = [col for col in date_cols if col in daily_df.columns]
                cum_df[f"{days}天累计"] = daily_df[valid_date_cols].sum(axis=1)
            
            # Save File
            daily_df.to_excel(daily_path, index=False, engine='openpyxl')
            cum_df.to_excel(cumulative_path, index=False, engine='openpyxl')
        
            logging.info(f"Successfully updated the summary file, which now contains data for{len(date_cols)}trading days.")
            logging.info(f"Daily Change File Index Count: {len(daily_df)}")
            logging.info(f"Cumulative Change File Index Count: {len(cum_df)}")
            return True
        
        except Exception as e:
            logging.error(f"Summary File Update Failure: {e}")
            import traceback
            logging.error(f"Detailed Error Information: {traceback.format_exc()}")
            return False
        
    def process_data(self):
        """Main Processing Flow (with Complete Validation)"""
        try:
            # 1. Data Retrieval
            today_df = self.fetch_data()
            if today_df.empty:
                raise ValueError("Retrieved Data is Empty")
            
            # 2. Save Raw Data
            if not self.save_daily_data(today_df):
                raise ValueError("Failed to Save Raw Data")
            # 3. Calculate Change
            changes_df = self.calculate_daily_changes(today_df)
            if changes_df is None or changes_df.empty:
                raise ValueError("Failed to Calculate Change")
            
            # 4. Index Aggregation
            aggregated = self.aggregate_by_index(changes_df)
            if aggregated is None or aggregated.empty:
                raise ValueError("Failed to Aggregate Index")
            
            # 5. Update Summary
            if not self.update_summary_files(aggregated):
                raise ValueError("Failed to Update Summary")
            
            # Print Results
            print("\n【Execution Results】")
            print(f"Process ETF Count: {len(today_df)}")
            print(f"Effective Index Count: {len(aggregated)}")
            print(f"Total AUM Change: {aggregated['规模变化'].sum():,.2f}")
            print("\nAUM ChangeTOP10:")
            print(aggregated.sort_values('规模变化', ascending=False)
                  .head(10)
                  .to_markdown(index=False))
            
            return True
            
        except Exception as e:
            logging.error(f"abnormal processing flow: {e}")
            return False

if __name__ == "__main__":
    # log configuration
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler("etf_crawler.log", encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logging.info("===== ETF data processor initialization =====")
    crawler = ETFCrawler()
    
    success = crawler.process_data()

    if not success:
        logging.error("!!! An error occurred during processing. Please check the logs !!!")
    
    logging.info("===== Processing completed =====")

